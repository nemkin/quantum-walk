\chapter{Introduction}

The following chapter contains the analysis of the TDK dissertation topic, historic background, motivation for the task: why it is important, relevant, useful, etc, the solutions currently available and then compare and contrast with the student's solution. Then, it concludes with the structure of the dissertation, describing the remaining chapters.

The following sections are based on:

Feng Xia, Senior Member, IEEE, Jiaying Liu, Hansong Nie, Yonghao Fu, Liangtian Wan, Member, IEEE,
Xiangjie Kong, Senior Member, IEEE - Random Walks: A Review of Algorithms and Applications, from IEEE Transactions on emerging topics in computational intelligence, on May 2019.

\section{Classical random walks on graphs}

Classical random walks are used to describe stochastic processes and many real life sciences rely on these methods. For example stock price movement prediction in finance, natural language processing algorithms, describing brownian motion in engineering physics, and various applications in biology and bioinformatics, such as DNA evolution models, population dynamics, modeling disease outbreaks, epidemic modeling ...etc. There are also algorithms, where stochastic processes are not directly present, but introduced as a way to deal with large quantities of data. Most notably, Google's Page Rank algorithm utilizes classical random walks on the large sized graph of the internet, to score documents on how good of an information source they are or how well they link to other information sources. Other algorithms include Markov chain Monte Carlo, for sampling from a probability distribution, difficult to directly model. Instead, they construct a stohastic process which's equilibrium distribution is the desired one and sample this by repeatedly recording states from the chain. The Metropolis Hashtings algorithm is similar to this, and is used to approximate the distribution or compute an integral (e.g expected value).

\unsure{Itt az lenne a célom hogy eladjam a véletlen sétákat, hogy ezek nagyon fontos algoritmusok. A Google PageRank egy nagyon jó példa.}

Random walks are effective in link prediction and recommendation system, computer vision, semi-supervised learning, network embedding, and complex
social network analysis. There are also some literature illustrating the applications of random walks on graphs, text analysis, science of science, and knowledge
discovery.

In conclusion, there are many important applications for classical random walks.

\section{From classical to quantum}

The scalable quantum computer is a topical issue so that approaches of quantum computation are popular topics nowadays. Quantum walks are the corresponding part of classical random walks in quantum mechanics. The main difference between them is that quantum walks don’t converge to some limiting distributions. Due to the quantum interference, quantum walks can spread significantly faster or slower than classical random walks. 

There are many properties of quantum walks that make them differ from their classical counterparts. While in classical walks, in N steps the walks reaches a distance of $O(\sqrt{N})$, in quantum, it is a lot faster, $O(N)$. Quantum walks have inherent interferences, where some amplitudes can amplify, while others can be destrutive to each other and diminish. This makes them behave very differently from classical random walks and a good reason to study them.

Quantum walks are often used to accelerate classical algorithms. It can be used to decision trees, search problems, and element distinctness.

There are many algorithms based on Quantum Walks. Two types of models are present: continuous time and discrete time walks. Quantum decision tree algorithm is one of the continuous time quantum walk based algorithm. In this algorithm, you systematically explore the decision tree as a graph. The decision tree nodes are quantum states in a Hilbert space. 

Based on Quantum Walks
\begin{itemize}
\item Quantum Decision Tree
\item Quantum PageRank
\item Grover Search Algorithm
\end{itemize}

\section{Applications}
\begin{itemize}
\item Collaborative Filtering
\item Recommender System
\item Link Prediction
\item Computer Vision
\item Semi-supervised
\item Learning
\item Network Embedding
\item Element Distinctness
\end{itemize}